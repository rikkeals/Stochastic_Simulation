{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e8d34d",
   "metadata": {},
   "source": [
    "## Øvelse 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2\n",
    "\n",
    "seed = 237\n",
    "\n",
    "# Parameters from Exercise 4\n",
    "A = 8\n",
    "m = 10\n",
    "num_samples = 10000\n",
    "burn_in = 0\n",
    "\n",
    "# Defining the unnormalized target distribution\n",
    "def target_pmf(i):\n",
    "    return (A ** i) / np.math.factorial(i)\n",
    "\n",
    "# Metropolis-Hastings algorithm\n",
    "def metropolis_hastings(A, m, num_samples, burn_in):\n",
    "    samples = []\n",
    "    current = np.random.randint(0, m+1)\n",
    "\n",
    "    for _ in range(num_samples + burn_in):\n",
    "        # Propose a new state\n",
    "        if current == 0:\n",
    "            proposal = current + 1\n",
    "        elif current == m:\n",
    "            proposal = current - 1\n",
    "        else:\n",
    "            proposal = current + np.random.choice([-1, 1])\n",
    "\n",
    "        # Computing acceptance ratio\n",
    "        ratio = target_pmf(proposal) / target_pmf(current)\n",
    "        alpha = min(1, ratio)\n",
    "\n",
    "        if np.random.rand() < alpha:\n",
    "            current = proposal\n",
    "\n",
    "        samples.append(current)\n",
    "\n",
    "    return np.array(samples[burn_in:])\n",
    "\n",
    "# Running the simulation\n",
    "samples = metropolis_hastings(A, m, num_samples, burn_in)\n",
    "\n",
    "# Hhistogram of the samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts, bins, patches = plt.hist(samples, bins=np.arange(m+2)-0.5, density=False, alpha=0.5, edgecolor='grey', color='skyblue')\n",
    "plt.xticks(np.arange(m+1))\n",
    "plt.title(\"Histogram of Metropolis-Hastings samples\", fontsize=16)\n",
    "plt.xlabel(\"State\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Chi test\n",
    "observed_counts = counts\n",
    "expected_probs = np.array([target_pmf(i) for i in range(m+1)])\n",
    "expected_probs /= expected_probs.sum()  # normalizing\n",
    "expected_counts = expected_probs * num_samples\n",
    "\n",
    "# Chi-square statistics\n",
    "chi_square_stat = ((observed_counts - expected_counts) ** 2 / expected_counts).sum()\n",
    "df = m  # degrees of freedom\n",
    "p_value = 1 - chi2.cdf(chi_square_stat, df)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi_square_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c2352",
   "metadata": {},
   "source": [
    "## Øvelse 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f22c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import factorial\n",
    "\n",
    "# Parameters\n",
    "A1 = A2 = 4\n",
    "m = 10\n",
    "num_samples = 10000\n",
    "burn_in = 0\n",
    "\n",
    "# Unnormalized joint distribution\n",
    "def target_pmf(i, j):\n",
    "    if i < 0 or j < 0 or i + j > m:\n",
    "        return 0\n",
    "    return (A1 ** i) / factorial(i) * (A2 ** j) / factorial(j)\n",
    "\n",
    "# Proposal: uniform move to one of the 8 neighboring states\n",
    "def propose(i, j):\n",
    "    moves = [(-1, 0), (1, 0), (0, -1), (0, 1),\n",
    "             (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "    np.random.shuffle(moves)\n",
    "    for di, dj in moves:\n",
    "        ni, nj = i + di, j + dj\n",
    "        if 0 <= ni <= m and 0 <= nj <= m and ni + nj <= m:\n",
    "            return ni, nj\n",
    "    return i, j  # if no valid move found\n",
    "\n",
    "# Metropolis-Hastings sampler\n",
    "def mh_joint_distribution(A1, A2, m, num_samples, burn_in):\n",
    "    samples = []\n",
    "    i, j = 0, 0  # initial state\n",
    "\n",
    "    for _ in range(num_samples + burn_in):\n",
    "        i_new, j_new = propose(i, j)\n",
    "        f_new = target_pmf(i_new, j_new)\n",
    "        f_old = target_pmf(i, j)\n",
    "\n",
    "        alpha = min(1, f_new / f_old) if f_old > 0 else 1\n",
    "\n",
    "        if np.random.rand() < alpha:\n",
    "            i, j = i_new, j_new\n",
    "\n",
    "        samples.append((i, j))\n",
    "\n",
    "    return np.array(samples[burn_in:])\n",
    "\n",
    "# Run the simulation\n",
    "samples_2a = mh_joint_distribution(A1, A2, m, num_samples, burn_in)\n",
    "\n",
    "# Plot a heatmap of frequencies\n",
    "heatmap = np.zeros((m+1, m+1))\n",
    "for i, j in samples_2a:\n",
    "    heatmap[i, j] += 1\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(heatmap, origin='lower', cmap='Blues', extent=[-0.5, m+0.5, -0.5, m+0.5])\n",
    "plt.colorbar(label='Frequency')\n",
    "plt.title('Joint Distribution Samples (Metropolis-Hastings)')\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('j')\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42d1e3",
   "metadata": {},
   "source": [
    "## Øvelse 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034582da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import factorial\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Parameters\n",
    "A1 = A2 = 4\n",
    "m = 10\n",
    "num_samples = 10000\n",
    "burn_in = 2000\n",
    "\n",
    "# Unnormalized joint distribution\n",
    "def target_pmf(i, j):\n",
    "    if i < 0 or j < 0 or i + j > m:\n",
    "        return 0\n",
    "    return (A1 ** i) / factorial(i) * (A2 ** j) / factorial(j)\n",
    "\n",
    "# Coordinate-wise Metropolis-Hastings sampler\n",
    "def mh_coordinatewise(A1, A2, m, num_samples, burn_in):\n",
    "    samples = []\n",
    "    i, j = 0, 0  # initial state\n",
    "\n",
    "    for _ in range(num_samples + burn_in):\n",
    "        # Step 1: Propose change in i\n",
    "        i_new = i + np.random.choice([-1, 1])\n",
    "        if 0 <= i_new <= m and i_new + j <= m:\n",
    "            alpha_i = min(1, target_pmf(i_new, j) / target_pmf(i, j))\n",
    "            if np.random.rand() < alpha_i:\n",
    "                i = i_new\n",
    "\n",
    "        # Step 2: Propose change in j\n",
    "        j_new = j + np.random.choice([-1, 1])\n",
    "        if 0 <= j_new <= m and i + j_new <= m:\n",
    "            alpha_j = min(1, target_pmf(i, j_new) / target_pmf(i, j))\n",
    "            if np.random.rand() < alpha_j:\n",
    "                j = j_new\n",
    "\n",
    "        samples.append((i, j))\n",
    "\n",
    "    return np.array(samples[burn_in:])\n",
    "\n",
    "# Compute expected frequencies for chi-squared test\n",
    "expected = np.zeros((m+1, m+1))\n",
    "Z = sum(target_pmf(i, j) for i in range(m + 1) for j in range(m + 1 - i))\n",
    "for i in range(m + 1):\n",
    "    for j in range(m + 1 - i):\n",
    "        expected[i, j] = target_pmf(i, j) / Z * num_samples\n",
    "\n",
    "# Run coordinate-wise MH\n",
    "samples_2b = mh_coordinatewise(A1, A2, m, num_samples, burn_in)\n",
    "\n",
    "# Count observed frequencies\n",
    "heatmap_2b = np.zeros((m+1, m+1))\n",
    "for i, j in samples_2b:\n",
    "    heatmap_2b[i, j] += 1\n",
    "\n",
    "# Compute chi-squared statistic\n",
    "chi2_stat_2b = 0\n",
    "df_2b = 0\n",
    "for i in range(m + 1):\n",
    "    for j in range(m + 1 - i):\n",
    "        if expected[i, j] >= 5:\n",
    "            obs = heatmap_2b[i, j]\n",
    "            exp = expected[i, j]\n",
    "            chi2_stat_2b += (obs - exp) ** 2 / exp\n",
    "            df_2b += 1\n",
    "\n",
    "p_value_2b = 1 - chi2.cdf(chi2_stat_2b, df_2b - 1)\n",
    "chi2_stat_2b, p_value_2b\n",
    "\n",
    "print(f\"Chi-squared: {chi2_stat_2b:.4f}\")\n",
    "print(f\"P-value: {p_value_2b:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48060cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c\n",
    "\n",
    "# Gibbs sampling implementation\n",
    "def gibbs_sampler(A1, A2, m, num_samples, burn_in):\n",
    "    samples = []\n",
    "    i, j = 0, 0  # Initial state\n",
    "\n",
    "    for _ in range(num_samples + burn_in):\n",
    "        # Sample i | j\n",
    "        max_i = m - j\n",
    "        weights_i = np.array([(A1**ii / factorial(ii)) for ii in range(max_i + 1)])\n",
    "        probs_i = weights_i / weights_i.sum()\n",
    "        i = np.random.choice(np.arange(max_i + 1), p=probs_i)\n",
    "\n",
    "        # Sample j | i\n",
    "        max_j = m - i\n",
    "        weights_j = np.array([(A2**jj / factorial(jj)) for jj in range(max_j + 1)])\n",
    "        probs_j = weights_j / weights_j.sum()\n",
    "        j = np.random.choice(np.arange(max_j + 1), p=probs_j)\n",
    "\n",
    "        samples.append((i, j))\n",
    "\n",
    "    return np.array(samples[burn_in:])\n",
    "\n",
    "# Run Gibbs sampler\n",
    "samples_2c = gibbs_sampler(A1, A2, m, num_samples, burn_in)\n",
    "\n",
    "# Count observed frequencies\n",
    "heatmap_2c = np.zeros((m+1, m+1))\n",
    "for i, j in samples_2c:\n",
    "    heatmap_2c[i, j] += 1\n",
    "\n",
    "# Compute chi-squared statistic\n",
    "chi2_stat_2c = 0\n",
    "df_2c = 0\n",
    "for i in range(m + 1):\n",
    "    for j in range(m + 1 - i):\n",
    "        if expected[i, j] >= 5:\n",
    "            obs = heatmap_2c[i, j]\n",
    "            exp = expected[i, j]\n",
    "            chi2_stat_2c += (obs - exp) ** 2 / exp\n",
    "            df_2c += 1\n",
    "\n",
    "p_value_2c = 1 - chi2.cdf(chi2_stat_2c, df_2c - 1)\n",
    "chi2_stat_2c, p_value_2c\n",
    "\n",
    "print(f\"Chi-squared (Gibbs): {chi2_stat_2c:.4f}\")\n",
    "print(f\"P-value (Gibbs): {p_value_2c:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2b00b",
   "metadata": {},
   "source": [
    "## Øvelse 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "# Parameters for the bivariate normal\n",
    "rho = 0.5\n",
    "mean = [0, 0]\n",
    "cov = [[1, rho], [rho, 1]]\n",
    "\n",
    "# Draw one sample from the bivariate normal distribution\n",
    "xi_gamma = np.random.multivariate_normal(mean, cov)\n",
    "\n",
    "# Transform to (theta, psi)\n",
    "theta = np.exp(xi_gamma[0])\n",
    "psi = np.exp(xi_gamma[1])\n",
    "\n",
    "theta, psi\n",
    "\n",
    "print(f\"Theta = {theta:.4f}\")\n",
    "print(f\"Psi = {psi:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "\n",
    "# Number of observations to generate\n",
    "n = 10\n",
    "\n",
    "# Generate n observations from N(theta, psi)\n",
    "observations = np.random.normal(loc=theta, scale=np.sqrt(psi), size=n)\n",
    "\n",
    "observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c\n",
    "\n",
    "# Use the same observations from before (as per image, hardcoded here)\n",
    "observations = np.array([\n",
    "    0.09117926, 0.95946243, 0.82744338, 1.00187944, -0.42340207,\n",
    "    -0.45253969, 0.56734008, -0.01968073, 2.40691926, -0.03388189\n",
    "])\n",
    "\n",
    "n = len(observations)\n",
    "x_bar = np.mean(observations)\n",
    "s2 = np.var(observations, ddof=0)\n",
    "\n",
    "print(f\"Sample mean (x̄) = {x_bar:.4f}\")\n",
    "print(f\"Sample variance (s²) = {s2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d\n",
    "\n",
    "# Posterior density function (up to proportionality)\n",
    "def posterior_density(theta, psi, x, rho=0.5):\n",
    "    if theta <= 0 or psi <= 0:\n",
    "        return 0\n",
    "\n",
    "    n = len(x)\n",
    "    x_bar = np.mean(x)\n",
    "    sum_sq = np.sum((x - theta)**2)\n",
    "\n",
    "    # Log-prior terms\n",
    "    log_theta = np.log(theta)\n",
    "    log_psi = np.log(psi)\n",
    "    exponent_prior = -(log_theta**2 - 2 * rho * log_theta * log_psi + log_psi**2) / (2 * (1 - rho**2))\n",
    "\n",
    "    # Log-likelihood + log-prior + Jacobian\n",
    "    log_post = (\n",
    "        -n / 2 * np.log(psi)\n",
    "        - sum_sq / (2 * psi)\n",
    "        - np.log(theta) - np.log(psi)\n",
    "        + exponent_prior\n",
    "    )\n",
    "    return np.exp(log_post)\n",
    "\n",
    "# Metropolis-Hastings sampling from the posterior\n",
    "def mh_posterior(x, n_samples=10000, burn_in=2000):\n",
    "    samples = []\n",
    "    theta, psi = 1.0, 1.0  # Initial values\n",
    "\n",
    "    for _ in range(n_samples + burn_in):\n",
    "        # Propose new values using log-normal random walk\n",
    "        theta_prop = np.random.lognormal(mean=np.log(theta), sigma=0.2)\n",
    "        psi_prop = np.random.lognormal(mean=np.log(psi), sigma=0.2)\n",
    "\n",
    "        # Acceptance ratio\n",
    "        p_current = posterior_density(theta, psi, x)\n",
    "        p_proposal = posterior_density(theta_prop, psi_prop, x)\n",
    "        alpha = min(1, p_proposal / p_current)\n",
    "\n",
    "        if np.random.rand() < alpha:\n",
    "            theta, psi = theta_prop, psi_prop\n",
    "\n",
    "        samples.append((theta, psi))\n",
    "\n",
    "    return np.array(samples[burn_in:])\n",
    "\n",
    "# Run the MH algorithm\n",
    "posterior_samples = mh_posterior(observations)\n",
    "theta_mean = np.mean(posterior_samples[:, 0])\n",
    "psi_mean = np.mean(posterior_samples[:, 1])\n",
    "\n",
    "print(f\"Posterior mean of Theta: {theta_mean:.4f}\")\n",
    "print(f\"Posterior mean of Psi: {psi_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e\n",
    "\n",
    "# Function to rerun MH for different sample sizes (n=100 and n=1000)\n",
    "def run_experiment(n, theta_true, psi_true):\n",
    "    # Simulate data from N(theta, psi)\n",
    "    x = np.random.normal(loc=theta_true, scale=np.sqrt(psi_true), size=n)\n",
    "    samples = mh_posterior(x)\n",
    "    theta_mean = np.mean(samples[:, 0])\n",
    "    psi_mean = np.mean(samples[:, 1])\n",
    "    return theta_mean, psi_mean\n",
    "\n",
    "# Reuse theta and psi from 3a\n",
    "theta_true = 0.5246\n",
    "psi_true = 0.3998\n",
    "\n",
    "# Run for n=100 and n=1000\n",
    "mean_theta_100, mean_psi_100 = run_experiment(100, theta_true, psi_true)\n",
    "mean_theta_1000, mean_psi_1000 = run_experiment(1000, theta_true, psi_true)\n",
    "\n",
    "(mean_theta_100, mean_psi_100), (mean_theta_1000, mean_psi_1000)\n",
    "print(f\"Mean Theta (n=100): {mean_theta_100:.4f}\")\n",
    "print(f\"Mean Psi (n=100): {mean_psi_100:.4f}\")\n",
    "print(f\"Mean Theta (n=1000): {mean_theta_1000:.4f}\")\n",
    "print(f\"Mean Psi (n=1000): {mean_psi_1000:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c8dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ecaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
